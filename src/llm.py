

import os
from typing import Dict, List
from dotenv import load_dotenv
from huggingface_hub import InferenceClient
import time
import json

load_dotenv()

HF_API_KEY = os.getenv("HF_API_KEY")

# Simple model dictionary - model_id is a STRING, not a list
FREE_MODELS = {
    "Llama-3.2-3B": "meta-llama/Llama-3.2-3B-Instruct",
    "Llama-3.2-1B": "meta-llama/Llama-3.2-1B-Instruct",
    "Qwen-2.5-7B": "Qwen/Qwen2.5-7B-Instruct",
    "Gemma-2B": "google/gemma-2-2b-it",                  # Safety-focused
}



from typing import Dict, List

# def merge_retrieval_results(results: Dict) -> Dict:
#     """
#     Merge and deduplicate results from baseline, minilm, and mpnet
#     """
#     all_nodes = {}
#     all_reviews = {}

#     for source_name, source_data in results.items():
#         if not source_data:
#             continue

#         # -------- Nodes --------
#         for node in source_data.get("nodes", []):
#             hotel_id = node.get("hotel_id")
#             if not hotel_id:
#                 continue

#             if hotel_id not in all_nodes:
#                 node_copy = node.copy()
#                 node_copy["id"] = hotel_id
#                 node_copy["source"] = source_name
#                 node_copy["reviews"] = []  # ADD THIS: Initialize reviews list
#                 all_nodes[hotel_id] = node_copy
#             else:
#                 existing = all_nodes[hotel_id]
#                 existing["source"] = "both"

#                 if "similarity_score" in node:
#                     best = max(
#                         existing.get("similarity_score", 0),
#                         node.get("similarity_score", 0)
#                     )
#                     existing["similarity_score"] = best

#         # -------- Reviews --------
#         # FIX: Associate reviews with their hotels
#         hotel_nodes = source_data.get("nodes", [])
#         if hotel_nodes:
#             # Assuming reviews in this source belong to the hotels in this source
#             hotel_id = hotel_nodes[0].get("hotel_id")  # Get the hotel from this source
            
#             for review in source_data.get("reviews", []):
#                 review_id = review.get("review_id")
#                 if review_id and review_id not in all_reviews:
#                     review_copy = review.copy()
#                     review_copy["hotel_id"] = hotel_id  # ADD hotel_id to review
#                     all_reviews[review_id] = review_copy
                    
#                     # Also add to the node's review list
#                     if hotel_id in all_nodes:
#                         all_nodes[hotel_id]["reviews"].append(review_copy)

#     return {
#         "nodes": list(all_nodes.values()),
#         "reviews": list(all_reviews.values()),
#         "total_sources": {
#             "baseline_only": sum(
#                 1 for n in all_nodes.values() if n["source"] == "baseline"
#             ),
#             "embeddings_only": sum(
#                 1 for n in all_nodes.values()
#                 if n["source"] in {"minilm", "mpnet"}
#             ),
#             "both": sum(
#                 1 for n in all_nodes.values() if n["source"] == "both"
#             ),
#         }
#     }


# def format_context(nodes: List[Dict], reviews: List[Dict]) -> str:
#     blocks = []

#     for node in nodes:
#         source = node.get("source", "unknown")
#         similarity = node.get("similarity_score")

#         source_info = f"[Source: {source}"
#         if similarity is not None:
#             source_info += f", similarity: {similarity:.2f}"
#         source_info += "]"

#         hotel_block = f"""Hotel ID: {node.get('id')}
# Name: {node.get('name')}
# City: {node.get('city')}, {node.get('country')}
# Stars: {node.get('star_rating')}
# {source_info}"""

#         # Add reviews for THIS hotel
#         hotel_reviews = node.get("reviews", [])
#         if hotel_reviews:
#             hotel_block += "\n\nReviews for this hotel:"
#             for r in hotel_reviews:
#                 hotel_block += f"""
#   - Review ID: {r.get('review_id')}
#     Text: "{r.get('review_text')}"
#     Overall Score: {r.get('score_overall')}/10
#     Date: {r.get('review_date')}"""
        
#         blocks.append(hotel_block)

#     return "\n\n".join(blocks)


def merge_retrieval_results(results: Dict) -> Dict:
    """
    Merge and deduplicate results from baseline, minilm, and mpnet
    """
    all_nodes = {}
    all_reviews = {}

    for source_name, source_data in results.items():
        if not source_data:
            continue

        # -------- Nodes --------
        for node in source_data.get("nodes", []):
            hotel_id = node.get("hotel_id")
            if not hotel_id:
                continue

            if hotel_id not in all_nodes:
                node_copy = node.copy()
                node_copy["id"] = hotel_id
                node_copy["source"] = source_name
                node_copy["reviews"] = []  # ADD THIS: Initialize reviews list
                all_nodes[hotel_id] = node_copy
            else:
                existing = all_nodes[hotel_id]
                existing["source"] = "both"

                if "similarity_score" in node:
                    best = max(
                        existing.get("similarity_score", 0),
                        node.get("similarity_score", 0)
                    )
                    existing["similarity_score"] = best

        # -------- Reviews --------
        # FIX: Associate reviews with their hotels
        hotel_nodes = source_data.get("nodes", [])
        if hotel_nodes:
            # Assuming reviews in this source belong to the hotels in this source
            hotel_id = hotel_nodes[0].get("hotel_id")  # Get the hotel from this source
            
            for review in source_data.get("reviews", []):
                review_id = review.get("review_id")
                if review_id and review_id not in all_reviews:
                    review_copy = review.copy()
                    review_copy["hotel_id"] = hotel_id  # ADD hotel_id to review
                    all_reviews[review_id] = review_copy
                    
                    # Also add to the node's review list
                    if hotel_id in all_nodes:
                        all_nodes[hotel_id]["reviews"].append(review_copy)

    return {
        "nodes": list(all_nodes.values()),
        "reviews": list(all_reviews.values()),
        "total_sources": {
            "baseline_only": sum(
                1 for n in all_nodes.values() if n["source"] == "baseline"
            ),
            "embeddings_only": sum(
                1 for n in all_nodes.values()
                if n["source"] in {"minilm", "mpnet"}
            ),
            "both": sum(
                1 for n in all_nodes.values() if n["source"] == "both"
            ),
        }
    }


def format_context(nodes: List[Dict], reviews: List[Dict]) -> str:
    blocks = []

    for node in nodes:
        source = node.get("source", "unknown")
        similarity = node.get("similarity_score")

        source_info = f"[Source: {source}"
        if similarity is not None:
            source_info += f", similarity: {similarity:.2f}"
        source_info += "]"

        hotel_block = f"""Hotel ID: {node.get('id')}
Name: {node.get('name')}
City: {node.get('city')}, {node.get('country')}
Stars: {node.get('star_rating')}
{source_info}"""

        # Add reviews for THIS hotel
        hotel_reviews = node.get("reviews", [])
        if hotel_reviews:
            hotel_block += "\n\nReviews for this hotel:"
            for r in hotel_reviews:
                hotel_block += f"""
  - Review ID: {r.get('review_id')}
    Text: "{r.get('review_text')}"
    Overall Score: {r.get('score_overall')}/10
    Date: {r.get('review_date')}"""
        
        blocks.append(hotel_block)

    return "\n\n".join(blocks)



def build_prompt(query: str, context: str) -> str:
    """Final optimized prompt - balanced and clear."""
    return f"""You are a helpful hotel information assistant.

HOTEL DATA:
{context}

IMPORTANT INSTRUCTIONS:
1. Answer in natural, conversational language (NOT as raw data or bullet points)
2. Use ONLY the information provided above - do not invent details
3. Do not mention amenities, landmarks, or nearby places unless stated in the data
4. Include all review scores with numbers (e.g., "8.9/10, 9.1/10, 9.0/10")
5. If review text seems unclear or unusual, briefly acknowledge it
6. Write 70-85 words in paragraph format

USER QUESTION: {query}

YOUR ANSWER (write naturally, like speaking to a traveler):"""

def evaluate_answer_quality(answer: str, query: str, context: str) -> Dict:
    """Comprehensive evaluation of answer quality."""
    metrics = {}
    
    # Basic metrics
    metrics["length_words"] = len(answer.split())
    metrics["length_chars"] = len(answer)
    
    # Extract hotel names from context
    context_hotels = set()
    for line in context.split('\n'):
        if line.startswith('Name:'):
            hotel_name = line.replace('Name:', '').strip()
            context_hotels.add(hotel_name.lower())
    
    # Check which hotels are mentioned in answer
    answer_lower = answer.lower()
    mentioned_hotels = [h for h in context_hotels if h in answer_lower]
    
    metrics["hotels_in_context"] = len(context_hotels)
    metrics["hotels_mentioned"] = len(mentioned_hotels)
    metrics["coverage"] = len(mentioned_hotels) / len(context_hotels) if context_hotels else 0
    
    # Query relevance
    query_terms = set(query.lower().split())
    answer_terms = set(answer.lower().split())
    overlap = query_terms & answer_terms
    metrics["query_relevance"] = len(overlap) / len(query_terms) if query_terms else 0
    
    # Quality indicators
    metrics["mentions_ratings"] = "rating" in answer.lower() or "/5" in answer
    metrics["mentions_amenities"] = any(word in answer.lower() for word in ['pool', 'wifi', 'breakfast', 'amenities'])
    metrics["provides_comparison"] = any(word in answer.lower() for word in ['both', 'compare', 'better', 'higher'])
    
    # Overall quality score
    quality_factors = [
        metrics["query_relevance"],
        metrics["coverage"],
        1.0 if metrics["mentions_ratings"] else 0.0,
        1.0 if metrics["mentions_amenities"] else 0.0,
    ]
    metrics["overall_quality"] = sum(quality_factors) / len(quality_factors)
    
    return metrics



def generate_answers_with_all_models(retrieval_result: Dict, retrieval_methods: list = None) -> Dict:
    """
    Generate answers using selected retrieval methods.
    
    Args:
        retrieval_result: Raw retrieval data with baseline, minilm, mpnet results
        retrieval_methods: List of methods to use. Options: ['baseline', 'minilm', 'mpnet']
                          If None, uses all available methods (hybrid)
    
    Returns:
        Dict with query, retrieval_stats, context, and model results
    """
    if not HF_API_KEY:
        raise ValueError("HF_API_KEY not found")

    # Default to all methods if not specified (hybrid approach)
    if retrieval_methods is None:
        retrieval_methods = ['baseline', 'minilm', 'mpnet']
    
    print(f"\nüîÑ Using retrieval methods: {', '.join(retrieval_methods)}")
    
    # Filter retrieval results based on selected methods
    filtered_results = {}
    all_results = retrieval_result.get("results", {})
    
    for method in retrieval_methods:
        if method in all_results:
            filtered_results[method] = all_results[method]
        else:
            print(f"   ‚ö†Ô∏è  Warning: {method} not found in retrieval results")
    
    if not filtered_results:
        raise ValueError(f"No valid retrieval methods found. Available: {list(all_results.keys())}")
    
    print(f"\nüîÑ Merging retrieval results from: {list(filtered_results.keys())}...")
    merged_data = merge_retrieval_results(filtered_results)

    print(f"   ‚úÖ Combined {len(merged_data['nodes'])} unique nodes")
    print(f"   üìä Sources: {merged_data['total_sources']}")

    context = format_context(
        merged_data["nodes"],
        merged_data["reviews"]
    )
    print("Context!!!")
    print(context)

    query = retrieval_result["query"]
    prompt = build_prompt(query, context)

    client = InferenceClient(token=HF_API_KEY)
    outputs = {}

    for model_name, model_id in FREE_MODELS.items():
        print(f"\nüîÑ Querying {model_name}...")
        try:
            start = time.time()

            response = client.chat_completion(
                model=model_id,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.1
            )

            answer = response.choices[0].message.content.strip()
            elapsed = time.time() - start

            metrics = evaluate_answer_quality(answer, query, context)

            outputs[model_name] = {
                "model_id": model_id,
                "answer": answer,
                "response_time": round(elapsed, 2),
                "quality_metrics": metrics,
                "status": "success"
            }

            print(f"‚úÖ {model_name} done ({elapsed:.2f}s)")

        except Exception as e:
            outputs[model_name] = {
                "model_id": model_id,
                "error": str(e),
                "status": "failed"
            }

        time.sleep(1)

    return {
        "query": query,
        "retrieval_methods_used": retrieval_methods,
        "retrieval_stats": {
            "total_nodes": len(merged_data["nodes"]),
            "total_reviews": len(merged_data["reviews"]),
            "source_breakdown": merged_data["total_sources"]
        },
        "context": context,
        "filtered_raw_data": filtered_results,  # Add filtered raw data for UI display
        "results": outputs
    }



def evaluate_answer_quality(answer: str, query: str, context: str) -> Dict:
    """
    Comprehensive evaluation with both quantitative and qualitative metrics.
    
    Quantitative: Length, coverage, relevance, token usage
    Qualitative: Correctness, naturalness, helpfulness
    """
    import re
    
    metrics = {}
    
    # ============================================
    # QUANTITATIVE METRICS
    # ============================================
    
    # 1. Length metrics
    metrics["length_words"] = len(answer.split())
    metrics["length_chars"] = len(answer)
    metrics["length_sentences"] = max(1, answer.count('.') + answer.count('!') + answer.count('?'))
    
    # 2. Token usage estimation (approximate: 1 token ‚âà 4 chars)
    metrics["estimated_tokens"] = len(answer) // 4
    
    # 3. Entity coverage (how many hotels mentioned)
    context_hotels = set()
    for line in context.split('\n'):
        if line.startswith('Name:'):
            hotel_name = line.replace('Name:', '').strip()
            context_hotels.add(hotel_name.lower())
    
    answer_lower = answer.lower()
    mentioned_hotels = [h for h in context_hotels if h in answer_lower]
    
    metrics["hotels_in_context"] = len(context_hotels)
    metrics["hotels_mentioned"] = len(mentioned_hotels)
    metrics["coverage"] = len(mentioned_hotels) / len(context_hotels) if context_hotels else 0
    
    # 4. Query relevance (keyword overlap)
    query_terms = set(query.lower().split())
    answer_terms = set(answer.lower().split())
    overlap = query_terms & answer_terms
    metrics["query_relevance"] = len(overlap) / len(query_terms) if query_terms else 0
    
    # 5. Information density (words per sentence)
    metrics["info_density"] = metrics["length_words"] / metrics["length_sentences"]
    
    # ============================================
    # QUALITATIVE INDICATORS (Automated)
    # ============================================
    
    # 6. Correctness indicators
    metrics["mentions_ratings"] = "rating" in answer.lower() or "/5" in answer
    metrics["mentions_location"] = any(word in answer.lower() for word in ['city', 'location', 'near', 'cairo', 'egypt'])
    metrics["provides_comparison"] = any(word in answer.lower() for word in ['both', 'compare', 'better', 'higher', 'vs', 'whereas'])
    
    # 7. Grounding check
    context_lower = context.lower()
    answer_numbers = re.findall(r'\b\d+\.?\d*\b', answer)
    context_numbers = re.findall(r'\b\d+\.?\d*\b', context)
    
    grounded_numbers = sum(1 for num in answer_numbers if num in context_numbers)
    metrics["grounding_score"] = grounded_numbers / len(answer_numbers) if answer_numbers else 1.0
    
    # 8. Naturalness indicators
    metrics["uses_natural_language"] = not answer.startswith('{') and not answer.startswith('[')
    metrics["has_conclusion"] = any(word in answer.lower() for word in ['therefore', 'overall', 'in summary', 'recommend', 'suggest'])
    metrics["conversational_tone"] = any(word in answer for word in ['I', 'you', 'your', 'would', 'should'])
    
    # 9. Helpfulness indicators
    metrics["actionable"] = any(word in answer.lower() for word in ['recommend', 'suggest', 'consider', 'choose', 'best'])
    metrics["provides_reasoning"] = any(word in answer.lower() for word in ['because', 'since', 'due to', 'as', 'given'])
    metrics["handles_uncertainty"] = any(phrase in answer.lower() for phrase in 
                                          ["don't have", "not available", "cannot determine", "unclear", "not enough"])
    
    # 10. Answer completeness
    if metrics["length_words"] < 10:
        metrics["completeness"] = "too_brief"
    elif metrics["length_words"] > 200:
        metrics["completeness"] = "too_verbose"
    else:
        metrics["completeness"] = "appropriate"
    
    # ============================================
    # OVERALL SCORES
    # ============================================
    
    # Accuracy score
    accuracy_factors = [
        metrics["grounding_score"],
        metrics["coverage"],
        1.0 if metrics["mentions_ratings"] else 0.5,
        1.0 if metrics["mentions_location"] else 0.5,
    ]
    metrics["accuracy_score"] = sum(accuracy_factors) / len(accuracy_factors)
    
    # Relevance score
    relevance_factors = [
        metrics["query_relevance"],
        metrics["coverage"],
        1.0 if metrics["actionable"] else 0.5,
    ]
    metrics["relevance_score"] = sum(relevance_factors) / len(relevance_factors)
    
    # Naturalness score
    naturalness_factors = [
        1.0 if metrics["uses_natural_language"] else 0.0,
        1.0 if metrics["conversational_tone"] else 0.5,
        1.0 if metrics["has_conclusion"] else 0.5,
        1.0 if metrics["completeness"] == "appropriate" else 0.5,
    ]
    metrics["naturalness_score"] = sum(naturalness_factors) / len(naturalness_factors)
    
    # Overall quality score (weighted average)
    metrics["overall_quality"] = (
        metrics["accuracy_score"] * 0.4 +
        metrics["relevance_score"] * 0.3 +
        metrics["naturalness_score"] * 0.3
    )
    
    return metrics

def print_detailed_comparison(result: Dict):
    """Print comprehensive comparison report with all metrics."""
    
    print("\n" + "="*80)
    print("COMPREHENSIVE LLM COMPARISON REPORT")
    print("="*80)
    
    print(f"\nüìù USER QUERY: {result['query']}")
    print(f"\nüìä RETRIEVAL STATISTICS:")
    stats = result['retrieval_stats']
    print(f"   Total Nodes Retrieved: {stats['total_nodes']}")
    print(f"   Total Reviews: {stats['total_reviews']}")
    print(f"   Source Breakdown:")
    for source, count in stats['source_breakdown'].items():
        print(f"      - {source}: {count}")
    
    print("\n" + "="*80)
    print("MODEL RESPONSES & ANALYSIS")
    print("="*80)
    
    successful = []
    
    for model_name, output in result["results"].items():
        print(f"\n{'‚îÄ'*80}")
        print(f"ü§ñ MODEL: {model_name}")
        print(f"   ID: {output['model_id']}")
        
        if output["status"] == "success":
            successful.append(model_name)
            metrics = output['quality_metrics']
            
            # ============================================
            # QUANTITATIVE METRICS
            # ============================================
            print(f"\nüìä QUANTITATIVE METRICS:")
            print(f"   Response Time: {output['response_time']}s")
            print(f"   Token Usage (estimated): ~{metrics['estimated_tokens']} tokens")
            print(f"   Answer Length: {metrics['length_words']} words, {metrics['length_sentences']} sentences")
            print(f"   Information Density: {metrics['info_density']:.1f} words/sentence")
            print(f"   Entity Coverage: {metrics['coverage']:.2%} ({metrics['hotels_mentioned']}/{metrics['hotels_in_context']} hotels)")
            print(f"   Query Relevance: {metrics['query_relevance']:.2%}")
            print(f"   Grounding Score: {metrics['grounding_score']:.2%}")
            
            # ============================================
            # QUALITATIVE INDICATORS
            # ============================================
            print(f"\nüìà QUALITATIVE INDICATORS:")
            print(f"   Accuracy Score: {metrics['accuracy_score']:.2%}")
            print(f"   Relevance Score: {metrics['relevance_score']:.2%}")
            print(f"   Naturalness Score: {metrics['naturalness_score']:.2%}")
            print(f"   Overall Quality: {metrics['overall_quality']:.2%}")
            
            print(f"\n‚úÖ CORRECTNESS:")
            print(f"   ‚Ä¢ Mentions Ratings: {'Yes' if metrics['mentions_ratings'] else 'No'}")
            print(f"   ‚Ä¢ Mentions Location: {'Yes' if metrics['mentions_location'] else 'No'}")
            print(f"   ‚Ä¢ Provides Comparison: {'Yes' if metrics['provides_comparison'] else 'No'}")
            print(f"   ‚Ä¢ Grounded in Context: {metrics['grounding_score']:.0%}")
            
            print(f"\nüí¨ NATURALNESS:")
            print(f"   ‚Ä¢ Natural Language: {'Yes' if metrics['uses_natural_language'] else 'No'}")
            print(f"   ‚Ä¢ Conversational Tone: {'Yes' if metrics['conversational_tone'] else 'No'}")
            print(f"   ‚Ä¢ Has Conclusion: {'Yes' if metrics['has_conclusion'] else 'No'}")
            print(f"   ‚Ä¢ Completeness: {metrics['completeness'].replace('_', ' ').title()}")
            
            print(f"\nüéØ HELPFULNESS:")
            print(f"   ‚Ä¢ Actionable Advice: {'Yes' if metrics['actionable'] else 'No'}")
            print(f"   ‚Ä¢ Provides Reasoning: {'Yes' if metrics['provides_reasoning'] else 'No'}")
            print(f"   ‚Ä¢ Handles Uncertainty: {'Yes' if metrics['handles_uncertainty'] else 'No'}")
            
            print(f"\nüìÑ ANSWER:")
            print(output["answer"])
        else:
            print(f"   ‚ùå ERROR: {output['error'][:200]}")
    
    # ============================================
    # COMPARATIVE ANALYSIS
    # ============================================
    if len(successful) >= 2:
        print("\n" + "="*80)
        print("COMPARATIVE ANALYSIS")
        print("="*80)
        
        # Speed ranking
        times = {m: result["results"][m]["response_time"] for m in successful}
        fastest = min(times, key=times.get)
        slowest = max(times, key=times.get)
        
        print(f"\n‚ö° SPEED RANKING:")
        sorted_by_speed = sorted(times.items(), key=lambda x: x[1])
        for i, (model, time_val) in enumerate(sorted_by_speed, 1):
            print(f"   {i}. {model}: {time_val}s")
        
        # Token usage ranking
        tokens = {m: result["results"][m]["quality_metrics"]["estimated_tokens"] for m in successful}
        most_tokens = max(tokens, key=tokens.get)
        least_tokens = min(tokens, key=tokens.get)
        
        print(f"\nüî¢ TOKEN USAGE:")
        sorted_by_tokens = sorted(tokens.items(), key=lambda x: x[1])
        for i, (model, token_count) in enumerate(sorted_by_tokens, 1):
            print(f"   {i}. {model}: ~{token_count} tokens")
        
        # Quality rankings
        print(f"\nüèÜ QUALITY RANKINGS:")
        
        # Overall quality
        qualities = {m: result["results"][m]["quality_metrics"]["overall_quality"] for m in successful}
        print(f"\n   Overall Quality:")
        sorted_by_quality = sorted(qualities.items(), key=lambda x: x[1], reverse=True)
        for i, (model, quality) in enumerate(sorted_by_quality, 1):
            print(f"   {i}. {model}: {quality:.2%}")
        best_quality = sorted_by_quality[0][0]
        
        # Accuracy
        accuracies = {m: result["results"][m]["quality_metrics"]["accuracy_score"] for m in successful}
        print(f"\n   Accuracy:")
        sorted_by_accuracy = sorted(accuracies.items(), key=lambda x: x[1], reverse=True)
        for i, (model, acc) in enumerate(sorted_by_accuracy, 1):
            print(f"   {i}. {model}: {acc:.2%}")
        most_accurate = sorted_by_accuracy[0][0]
        
        # Relevance
        relevances = {m: result["results"][m]["quality_metrics"]["relevance_score"] for m in successful}
        print(f"\n   Relevance:")
        sorted_by_relevance = sorted(relevances.items(), key=lambda x: x[1], reverse=True)
        for i, (model, rel) in enumerate(sorted_by_relevance, 1):
            print(f"   {i}. {model}: {rel:.2%}")
        most_relevant = sorted_by_relevance[0][0]
        
        # Naturalness
        naturalnesses = {m: result["results"][m]["quality_metrics"]["naturalness_score"] for m in successful}
        print(f"\n   Naturalness:")
        sorted_by_naturalness = sorted(naturalnesses.items(), key=lambda x: x[1], reverse=True)
        for i, (model, nat) in enumerate(sorted_by_naturalness, 1):
            print(f"   {i}. {model}: {nat:.2%}")
        most_natural = sorted_by_naturalness[0][0]
        
        # ============================================
        # COST ANALYSIS (Estimated)
        # ============================================
        print(f"\nüí∞ ESTIMATED COST ANALYSIS (if using paid API):")
        print(f"   Note: These are FREE on HuggingFace, but here's what it would cost elsewhere:")
        
        # Typical pricing: ~$0.10-0.50 per 1M tokens for small models
        for model in successful:
            token_count = result["results"][model]["quality_metrics"]["estimated_tokens"]
            # Rough estimate: smaller models = cheaper
            if "1B" in model:
                cost_per_1m = 0.10
            elif "3B" in model:
                cost_per_1m = 0.20
            else:  # 7B+
                cost_per_1m = 0.50
            
            estimated_cost = (token_count / 1_000_000) * cost_per_1m
            print(f"   {model}: ${estimated_cost:.6f} per query (~${estimated_cost * 1000:.2f} per 1K queries)")
        
        # ============================================
        # RECOMMENDATIONS
        # ============================================
        print(f"\nüéØ RECOMMENDATIONS:")
        print(f"\n   Best Overall: {best_quality}")
        print(f"   Fastest: {fastest} ({times[fastest]:.2f}s)")
        print(f"   Most Accurate: {most_accurate} ({accuracies[most_accurate]:.2%})")
        print(f"   Most Relevant: {most_relevant} ({relevances[most_relevant]:.2%})")
        print(f"   Most Natural: {most_natural} ({naturalnesses[most_natural]:.2%})")
        print(f"   Most Efficient (tokens): {least_tokens} (~{tokens[least_tokens]} tokens)")
        
        print(f"\n   üí° USE CASE RECOMMENDATIONS:")
        print(f"   ‚Ä¢ Production (balanced): {best_quality}")
        print(f"   ‚Ä¢ Speed-critical: {fastest}")
        print(f"   ‚Ä¢ Quality-critical: {most_accurate}")
        print(f"   ‚Ä¢ Cost-sensitive: {least_tokens}")
        
        # Trade-off analysis
        print(f"\n   ‚öñÔ∏è  TRADE-OFF ANALYSIS:")
        if fastest == best_quality:
            print(f"   ‚ú® {fastest} is both fastest AND highest quality - IDEAL!")
        else:
            speed_diff = times[slowest] - times[fastest]
            quality_diff = qualities[best_quality] - qualities[fastest]
            print(f"   ‚Ä¢ {best_quality} is {quality_diff:.1%} better quality but {speed_diff:.2f}s slower")
            print(f"   ‚Ä¢ {fastest} is {speed_diff:.2f}s faster but {quality_diff:.1%} lower quality")
            
            if quality_diff < 0.10:  # Less than 10% quality difference
                print(f"   ‚û°Ô∏è  Recommendation: Use {fastest} (quality difference is minimal)")
            else:
                print(f"   ‚û°Ô∏è  Recommendation: Use {best_quality} for quality, {fastest} for speed")


def get_ui_response(result: Dict) -> list:
    """
    Returns a simple list of dictionaries with model names and responses.
    
    Returns:
        List of dicts: [{"model_name": str, "response": str}, ...]
    """
    models = []
    
    for model_name, output in result["results"].items():
        if output["status"] == "success":
            models.append({
                "model_name": model_name,
                "response": output["answer"]
            })
        else:
            models.append({
                "model_name": model_name,
                "response": f"Error: {output['error']}"
            })
    
    return models


# ## üìã **Complete Metrics Summary**

# ### **Quantitative Metrics** ‚úÖ
# | Metric | What it Measures | How |
# |--------|------------------|-----|
# | **Response Time** | Speed of answer generation | Actual time measurement |
# | **Token Usage** | API cost/resource usage | Character count / 4 |
# | **Answer Length** | Verbosity (words, sentences) | Word/sentence count |
# | **Information Density** | Words per sentence | Words / sentences |
# | **Entity Coverage** | How many hotels mentioned | Hotels mentioned / total |
# | **Query Relevance** | Keyword overlap with query | Jaccard similarity |
# | **Grounding Score** | Factual accuracy | Numbers in answer found in context |

# ### **Qualitative Metrics** ‚úÖ
# | Metric | What it Measures | How |
# |--------|------------------|-----|
# | **Accuracy Score** | Correctness of information | Grounding + coverage + facts |
# | **Relevance Score** | How well it answers query | Query relevance + actionability |
# | **Naturalness Score** | Human-like quality | Conversational + completeness |
# | **Overall Quality** | Weighted combination | 40% accuracy + 30% relevance + 30% naturalness |
# | **Correctness Indicators** | Mentions ratings, locations, etc | Keyword detection |
# | **Naturalness Indicators** | Conversational tone, conclusions | Pattern matching |
# | **Helpfulness Indicators** | Actionable advice, reasoning | Keyword detection |

# ---

# ## üéØ **Sample Output**
# ```
# ================================================================================
# COMPREHENSIVE LLM COMPARISON REPORT
# ================================================================================

# üìù USER QUERY: Find hotels in Cairo with pools

# üìä RETRIEVAL STATISTICS:
#    Total Nodes Retrieved: 3
#    Total Reviews: 2
#    Source Breakdown:
#       - baseline_only: 1
#       - embeddings_only: 1
#       - both: 1

# ================================================================================
# MODEL RESPONSES & ANALYSIS
# ================================================================================

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ü§ñ MODEL: Llama-3.2-3B
#    ID: meta-llama/Llama-3.2-3B-Instruct

# üìä QUANTITATIVE METRICS:
#    Response Time: 1.23s
#    Token Usage (estimated): ~187 tokens
#    Answer Length: 75 words, 4 sentences
#    Information Density: 18.8 words/sentence
#    Entity Coverage: 66.67% (2/3 hotels)
#    Query Relevance: 75.00%
#    Grounding Score: 100.00%

# üìà QUALITATIVE INDICATORS:
#    Accuracy Score: 87.50%
#    Relevance Score: 80.56%
#    Naturalness Score: 87.50%
#    Overall Quality: 85.42%

# ‚úÖ CORRECTNESS:
#    ‚Ä¢ Mentions Ratings: Yes
#    ‚Ä¢ Mentions Location: Yes
#    ‚Ä¢ Provides Comparison: Yes
#    ‚Ä¢ Grounded in Context: 100%

# üí¨ NATURALNESS:
#    ‚Ä¢ Natural Language: Yes
#    ‚Ä¢ Conversational Tone: Yes
#    ‚Ä¢ Has Conclusion: Yes
#    ‚Ä¢ Completeness: Appropriate

# üéØ HELPFULNESS:
#    ‚Ä¢ Actionable Advice: Yes
#    ‚Ä¢ Provides Reasoning: Yes
#    ‚Ä¢ Handles Uncertainty: No

# üìÑ ANSWER:
# Based on the retrieved information, I found two hotels in Cairo with pools:
# Nile Plaza (4.5/5 rating) and Cairo Grand (4.3/5). Nile Plaza has a higher
# rating and is located near the Egyptian Museum. I recommend Nile Plaza for
# its excellent rating and convenient location.

# ================================================================================
# COMPARATIVE ANALYSIS
# ================================================================================

# ‚ö° SPEED RANKING:
#    1. Llama-3.2-1B: 0.87s
#    2. Llama-3.2-3B: 1.23s
#    3. Qwen-2.5-7B: 3.45s

# üî¢ TOKEN USAGE:
#    1. Llama-3.2-1B: ~142 tokens
#    2. Llama-3.2-3B: ~187 tokens
#    3. Qwen-2.5-7B: ~215 tokens

# üèÜ QUALITY RANKINGS:

#    Overall Quality:
#    1. Qwen-2.5-7B: 91.23%
#    2. Llama-3.2-3B: 85.42%
#    3. Llama-3.2-1B: 78.34%

#    Accuracy:
#    1. Qwen-2.5-7B: 93.75%
#    2. Llama-3.2-3B: 87.50%
#    3. Llama-3.2-1B: 81.25%

#    Relevance:
#    1. Qwen-2.5-7B: 88.89%
#    2. Llama-3.2-3B: 80.56%
#    3. Llama-3.2-1B: 75.00%

#    Naturalness:
#    1. Llama-3.2-3B: 87.50%
#    2. Qwen-2.5-7B: 87.50%
#    3. Llama-3.2-1B: 75.00%

# # üí∞ ESTIMATED COST ANALYSIS (if using paid API):
# #    Note: These are FREE on HuggingFace, but here's what it would cost elsewhere:
# #    Llama-3.2-1B: $0.000014 per query (~$0.01 per 1K queries)
# #    Llama-3.2-3B: $0.000037 per query (~$0.04 per 1K queries)
# #    Qwen-2.5-7B: $0.000108 per query (~$0.11 per 1K queries)

# üéØ RECOMMENDATIONS:

#    Best Overall: Qwen-2.5-7B
#    Fastest: Llama-3.2-1B (0.87s)
#    Most Accurate: Qwen-2.5-7B (93.75%)
#    Most Relevant: Qwen-2.5-7B (88.89%)
#    Most Natural: Llama-3.2-3B (87.50%)
#    Most Efficient (tokens): Llama-3.2-1B (~142 tokens)

#    üí° USE CASE RECOMMENDATIONS:
#    ‚Ä¢ Production (balanced): Qwen-2.5-7B
#    ‚Ä¢ Speed-critical: Llama-3.2-1B
#    ‚Ä¢ Quality-critical: Qwen-2.5-7B
#    ‚Ä¢ Cost-sensitive: Llama-3.2-1B

#    ‚öñÔ∏è  TRADE-OFF ANALYSIS:
#    ‚Ä¢ Qwen-2.5-7B is 5.8% better quality but 2.58s slower
#    ‚Ä¢ Llama-3.2-1B is 2.58s faster but 5.8% lower quality
#    ‚û°Ô∏è  Recommendation: Use Qwen-2.5-7B for quality, Llama-3.2-1B for speed

def get_ui_response(result: Dict) -> list:
    """
    Returns a simple list of dictionaries with model names and responses.
    
    Returns:
        List of dicts: [{"model_name": str, "response": str}, ...]
    """
    models = []
    
    for model_name, output in result["results"].items():
        if output["status"] == "success":
            models.append({
                "model_name": model_name,
                "response": output["answer"]
            })
        else:
            models.append({
                "model_name": model_name,
                "response": f"Error: {output['error']}"
            })
    
    return models
if __name__ == "__main__":
    # Test data
    from example_retrieval_result import example_retrieval_result
    
    print("Starting Task 3: LLM Layer")
    print(f"HuggingFace API Key: {'‚úÖ Found' if HF_API_KEY else '‚ùå Missing'}")
    
    if not HF_API_KEY:
        print("\n‚ö†Ô∏è  ERROR: Set HF_API_KEY in .env file")
        exit(1)
    
    result = generate_answers_with_all_models(example_retrieval_result)
    print(result)
    print_detailed_comparison(result)

    # models_data = get_ui_response(result)
    # print(models_data)
    # ui_result=get_ui_response(result)
    # print(ui_result)
    

    

    # print_detailed_comparison(result)
